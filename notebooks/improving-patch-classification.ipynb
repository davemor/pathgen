{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conceptual-large",
   "metadata": {},
   "source": [
    "# Improving Patch Classification\n",
    "In this notebook, I'm going to explore how to improve the patch classifier as the current one is not really performing. In fact let's try out a bunch of the pretrained ones from torch vision. We have quite large datasets so it makes sense to try and train the whole network, not just the end layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broke-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard project preamble\n",
    "from pathgen.utils.seeds import set_seed\n",
    "from pathgen.utils.paths import project_root\n",
    "\n",
    "experiment_name = \"all\"\n",
    "experiment_root = project_root() / \"experiments\" / experiment_name\n",
    "\n",
    "global_seed = 987654321\n",
    "set_seed(global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard data science imports\n",
    "import numpy as np\n",
    "\n",
    "# standard pytorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# pytorch data loading imports\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concerned-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our hyper parameters\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "learning_rate = 0.00001  # 0.001 - takes 25mins (log10 grid search?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occupational-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cooperative-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the data loaders for training\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])  # these values are what the pretrained models expect\n",
    "])\n",
    "\n",
    "train_set = ImageFolder(experiment_root / 'train_patches', transform=transform)  # 70,000 samples (35,000 per class) - non-fake\n",
    "valid_set = ImageFolder(experiment_root / 'valid_patches', transform=transform)  # 30,000 samples (15,000 per class) - non-fake\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=np.random.seed(global_seed), num_workers=32)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, worker_init_fn=np.random.seed(global_seed), num_workers=32)\n",
    "\n",
    "# downsample to speed up training to test the code\n",
    "#train_sampler = RandomSampler(train_set, replacement=True, num_samples=256)\n",
    "#valid_sampler = RandomSampler(valid_set, replacement=True, num_samples=256)\n",
    "\n",
    "#train_loader = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, worker_init_fn=np.random.seed(global_seed), num_workers=32)\n",
    "#valid_loader = DataLoader(valid_set, batch_size=batch_size, sampler=valid_sampler, worker_init_fn=np.random.seed(global_seed), num_workers=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-minister",
   "metadata": {},
   "source": [
    "## Saving and loading models\n",
    "Let's add some functions for loading an saving the state of the model and training using a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, path):\n",
    "    print(f\"saving checkpoint to {path}\")\n",
    "    state = { 'epoch': epoch, \n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict() }\n",
    "    torch.save(state, path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    print(\"loading checkpoint\")\n",
    "    state = torch.load(path)\n",
    "    epoch = state[\"epoch\"]\n",
    "    model.load_state_dict(state[\"state_dict\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    return epoch, model, optimizer, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-chart",
   "metadata": {},
   "source": [
    "## Set up the models that we are going to test\n",
    "We are going to test using VGG16, Resnet-18, InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "premium-running",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2796,  0.0056, -0.3369,  ..., -0.5082, -0.2513, -0.2171],\n",
       "         [ 0.5364,  0.3138,  0.0227,  ..., -0.3027, -0.0629,  0.0398],\n",
       "         [ 0.9474,  0.7248,  0.6049,  ...,  0.7248,  1.0159,  1.2043],\n",
       "         ...,\n",
       "         [ 2.2489,  2.2489,  2.2489,  ...,  1.1700, -0.0287, -0.8507],\n",
       "         [ 2.0948,  2.1804,  2.2147,  ...,  1.3413,  0.2453, -0.6281],\n",
       "         [ 1.9235,  2.0092,  2.1462,  ...,  1.6667,  0.6392, -0.2342]],\n",
       "\n",
       "        [[-0.3375, -0.6176, -1.0203,  ..., -0.9678, -0.7052, -0.7052],\n",
       "         [-0.0399, -0.3025, -0.6001,  ..., -0.6702, -0.4426, -0.3901],\n",
       "         [ 0.4328,  0.2052,  0.0301,  ...,  0.4853,  0.7654,  0.9580],\n",
       "         ...,\n",
       "         [ 2.2010,  2.2360,  2.1485,  ...,  0.8004, -0.4426, -1.3004],\n",
       "         [ 2.3060,  2.3585,  2.3585,  ...,  0.9930, -0.1275, -1.0378],\n",
       "         [ 2.2010,  2.2710,  2.3585,  ...,  1.2731,  0.2402, -0.7052]],\n",
       "\n",
       "        [[ 0.6705,  0.3916,  0.0605,  ...,  0.0779,  0.4439,  0.4614],\n",
       "         [ 0.9145,  0.6705,  0.4265,  ...,  0.2522,  0.5485,  0.6531],\n",
       "         [ 1.2980,  1.0714,  0.9842,  ...,  1.1062,  1.4722,  1.6814],\n",
       "         ...,\n",
       "         [ 2.5180,  2.5529,  2.5180,  ...,  1.5071,  0.3742, -0.3927],\n",
       "         [ 2.4657,  2.5354,  2.5877,  ...,  1.5245,  0.4788, -0.3230],\n",
       "         [ 2.3437,  2.4134,  2.5529,  ...,  1.7163,  0.7228, -0.0615]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preamble\n",
    "from torchsummary import summary\n",
    "\n",
    "# get and example image from the dataset so we can generate the summaries\n",
    "img, _ = train_set[0]\n",
    "img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "universal-prefix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 256, 256]           1,792\n",
      "              ReLU-2        [128, 64, 256, 256]               0\n",
      "            Conv2d-3        [128, 64, 256, 256]          36,928\n",
      "              ReLU-4        [128, 64, 256, 256]               0\n",
      "         MaxPool2d-5        [128, 64, 128, 128]               0\n",
      "            Conv2d-6       [128, 128, 128, 128]          73,856\n",
      "              ReLU-7       [128, 128, 128, 128]               0\n",
      "            Conv2d-8       [128, 128, 128, 128]         147,584\n",
      "              ReLU-9       [128, 128, 128, 128]               0\n",
      "        MaxPool2d-10         [128, 128, 64, 64]               0\n",
      "           Conv2d-11         [128, 256, 64, 64]         295,168\n",
      "             ReLU-12         [128, 256, 64, 64]               0\n",
      "           Conv2d-13         [128, 256, 64, 64]         590,080\n",
      "             ReLU-14         [128, 256, 64, 64]               0\n",
      "           Conv2d-15         [128, 256, 64, 64]         590,080\n",
      "             ReLU-16         [128, 256, 64, 64]               0\n",
      "        MaxPool2d-17         [128, 256, 32, 32]               0\n",
      "           Conv2d-18         [128, 512, 32, 32]       1,180,160\n",
      "             ReLU-19         [128, 512, 32, 32]               0\n",
      "           Conv2d-20         [128, 512, 32, 32]       2,359,808\n",
      "             ReLU-21         [128, 512, 32, 32]               0\n",
      "           Conv2d-22         [128, 512, 32, 32]       2,359,808\n",
      "             ReLU-23         [128, 512, 32, 32]               0\n",
      "        MaxPool2d-24         [128, 512, 16, 16]               0\n",
      "           Conv2d-25         [128, 512, 16, 16]       2,359,808\n",
      "             ReLU-26         [128, 512, 16, 16]               0\n",
      "           Conv2d-27         [128, 512, 16, 16]       2,359,808\n",
      "             ReLU-28         [128, 512, 16, 16]               0\n",
      "           Conv2d-29         [128, 512, 16, 16]       2,359,808\n",
      "             ReLU-30         [128, 512, 16, 16]               0\n",
      "        MaxPool2d-31           [128, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-32           [128, 512, 7, 7]               0\n",
      "           Linear-33                [128, 4096]     102,764,544\n",
      "             ReLU-34                [128, 4096]               0\n",
      "          Dropout-35                [128, 4096]               0\n",
      "           Linear-36                [128, 4096]      16,781,312\n",
      "             ReLU-37                [128, 4096]               0\n",
      "          Dropout-38                [128, 4096]               0\n",
      "           Linear-39                   [128, 1]           4,097\n",
      "================================================================\n",
      "Total params: 134,264,641\n",
      "Trainable params: 134,264,641\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 96.00\n",
      "Forward/backward pass size (MB): 36560.50\n",
      "Params size (MB): 512.18\n",
      "Estimated Total Size (MB): 37168.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# VGG16 load the model\n",
    "model_vgg16 = torchvision.models.vgg16(pretrained=False)\n",
    "model_vgg16.classifier[6] = nn.Sequential(nn.Linear(in_features=4096, out_features=1, bias=True))\n",
    "\n",
    "# get a summary of the model based on our input shape\n",
    "model_vgg16.to(device)\n",
    "summary(model_vgg16, img.shape, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fossil-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2        [128, 64, 128, 128]             128\n",
      "              ReLU-3        [128, 64, 128, 128]               0\n",
      "         MaxPool2d-4          [128, 64, 64, 64]               0\n",
      "            Conv2d-5          [128, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-6          [128, 64, 64, 64]             128\n",
      "              ReLU-7          [128, 64, 64, 64]               0\n",
      "            Conv2d-8          [128, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9          [128, 64, 64, 64]             128\n",
      "             ReLU-10          [128, 64, 64, 64]               0\n",
      "       BasicBlock-11          [128, 64, 64, 64]               0\n",
      "           Conv2d-12          [128, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-13          [128, 64, 64, 64]             128\n",
      "             ReLU-14          [128, 64, 64, 64]               0\n",
      "           Conv2d-15          [128, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-16          [128, 64, 64, 64]             128\n",
      "             ReLU-17          [128, 64, 64, 64]               0\n",
      "       BasicBlock-18          [128, 64, 64, 64]               0\n",
      "           Conv2d-19         [128, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-20         [128, 128, 32, 32]             256\n",
      "             ReLU-21         [128, 128, 32, 32]               0\n",
      "           Conv2d-22         [128, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-23         [128, 128, 32, 32]             256\n",
      "           Conv2d-24         [128, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-25         [128, 128, 32, 32]             256\n",
      "             ReLU-26         [128, 128, 32, 32]               0\n",
      "       BasicBlock-27         [128, 128, 32, 32]               0\n",
      "           Conv2d-28         [128, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-29         [128, 128, 32, 32]             256\n",
      "             ReLU-30         [128, 128, 32, 32]               0\n",
      "           Conv2d-31         [128, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-32         [128, 128, 32, 32]             256\n",
      "             ReLU-33         [128, 128, 32, 32]               0\n",
      "       BasicBlock-34         [128, 128, 32, 32]               0\n",
      "           Conv2d-35         [128, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-36         [128, 256, 16, 16]             512\n",
      "             ReLU-37         [128, 256, 16, 16]               0\n",
      "           Conv2d-38         [128, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-39         [128, 256, 16, 16]             512\n",
      "           Conv2d-40         [128, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-41         [128, 256, 16, 16]             512\n",
      "             ReLU-42         [128, 256, 16, 16]               0\n",
      "       BasicBlock-43         [128, 256, 16, 16]               0\n",
      "           Conv2d-44         [128, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45         [128, 256, 16, 16]             512\n",
      "             ReLU-46         [128, 256, 16, 16]               0\n",
      "           Conv2d-47         [128, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-48         [128, 256, 16, 16]             512\n",
      "             ReLU-49         [128, 256, 16, 16]               0\n",
      "       BasicBlock-50         [128, 256, 16, 16]               0\n",
      "           Conv2d-51           [128, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-52           [128, 512, 8, 8]           1,024\n",
      "             ReLU-53           [128, 512, 8, 8]               0\n",
      "           Conv2d-54           [128, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-55           [128, 512, 8, 8]           1,024\n",
      "           Conv2d-56           [128, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-57           [128, 512, 8, 8]           1,024\n",
      "             ReLU-58           [128, 512, 8, 8]               0\n",
      "       BasicBlock-59           [128, 512, 8, 8]               0\n",
      "           Conv2d-60           [128, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61           [128, 512, 8, 8]           1,024\n",
      "             ReLU-62           [128, 512, 8, 8]               0\n",
      "           Conv2d-63           [128, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-64           [128, 512, 8, 8]           1,024\n",
      "             ReLU-65           [128, 512, 8, 8]               0\n",
      "       BasicBlock-66           [128, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-67           [128, 512, 1, 1]               0\n",
      "           Linear-68                   [128, 1]             513\n",
      "================================================================\n",
      "Total params: 11,177,025\n",
      "Trainable params: 11,177,025\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 96.00\n",
      "Forward/backward pass size (MB): 10496.50\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 10635.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load in the pretrained resnet18 model and change the number of outputs on the final fc layer\n",
    "model_resnet18 = torchvision.models.resnet18(pretrained=False)\n",
    "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, 1)\n",
    "\n",
    "# get a summary of the model based on our input shape\n",
    "model_resnet18.to(device)\n",
    "print(model_resnet18)\n",
    "summary(model_resnet18, img.shape, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "operating-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2        [128, 64, 128, 128]             128\n",
      "       BasicConv2d-3        [128, 64, 128, 128]               0\n",
      "         MaxPool2d-4          [128, 64, 64, 64]               0\n",
      "            Conv2d-5          [128, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-6          [128, 64, 64, 64]             128\n",
      "       BasicConv2d-7          [128, 64, 64, 64]               0\n",
      "            Conv2d-8         [128, 192, 64, 64]         110,592\n",
      "       BatchNorm2d-9         [128, 192, 64, 64]             384\n",
      "      BasicConv2d-10         [128, 192, 64, 64]               0\n",
      "        MaxPool2d-11         [128, 192, 32, 32]               0\n",
      "           Conv2d-12          [128, 64, 32, 32]          12,288\n",
      "      BatchNorm2d-13          [128, 64, 32, 32]             128\n",
      "      BasicConv2d-14          [128, 64, 32, 32]               0\n",
      "           Conv2d-15          [128, 96, 32, 32]          18,432\n",
      "      BatchNorm2d-16          [128, 96, 32, 32]             192\n",
      "      BasicConv2d-17          [128, 96, 32, 32]               0\n",
      "           Conv2d-18         [128, 128, 32, 32]         110,592\n",
      "      BatchNorm2d-19         [128, 128, 32, 32]             256\n",
      "      BasicConv2d-20         [128, 128, 32, 32]               0\n",
      "           Conv2d-21          [128, 16, 32, 32]           3,072\n",
      "      BatchNorm2d-22          [128, 16, 32, 32]              32\n",
      "      BasicConv2d-23          [128, 16, 32, 32]               0\n",
      "           Conv2d-24          [128, 32, 32, 32]           4,608\n",
      "      BatchNorm2d-25          [128, 32, 32, 32]              64\n",
      "      BasicConv2d-26          [128, 32, 32, 32]               0\n",
      "        MaxPool2d-27         [128, 192, 32, 32]               0\n",
      "           Conv2d-28          [128, 32, 32, 32]           6,144\n",
      "      BatchNorm2d-29          [128, 32, 32, 32]              64\n",
      "      BasicConv2d-30          [128, 32, 32, 32]               0\n",
      "        Inception-31         [128, 256, 32, 32]               0\n",
      "           Conv2d-32         [128, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-33         [128, 128, 32, 32]             256\n",
      "      BasicConv2d-34         [128, 128, 32, 32]               0\n",
      "           Conv2d-35         [128, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-36         [128, 128, 32, 32]             256\n",
      "      BasicConv2d-37         [128, 128, 32, 32]               0\n",
      "           Conv2d-38         [128, 192, 32, 32]         221,184\n",
      "      BatchNorm2d-39         [128, 192, 32, 32]             384\n",
      "      BasicConv2d-40         [128, 192, 32, 32]               0\n",
      "           Conv2d-41          [128, 32, 32, 32]           8,192\n",
      "      BatchNorm2d-42          [128, 32, 32, 32]              64\n",
      "      BasicConv2d-43          [128, 32, 32, 32]               0\n",
      "           Conv2d-44          [128, 96, 32, 32]          27,648\n",
      "      BatchNorm2d-45          [128, 96, 32, 32]             192\n",
      "      BasicConv2d-46          [128, 96, 32, 32]               0\n",
      "        MaxPool2d-47         [128, 256, 32, 32]               0\n",
      "           Conv2d-48          [128, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-49          [128, 64, 32, 32]             128\n",
      "      BasicConv2d-50          [128, 64, 32, 32]               0\n",
      "        Inception-51         [128, 480, 32, 32]               0\n",
      "        MaxPool2d-52         [128, 480, 16, 16]               0\n",
      "           Conv2d-53         [128, 192, 16, 16]          92,160\n",
      "      BatchNorm2d-54         [128, 192, 16, 16]             384\n",
      "      BasicConv2d-55         [128, 192, 16, 16]               0\n",
      "           Conv2d-56          [128, 96, 16, 16]          46,080\n",
      "      BatchNorm2d-57          [128, 96, 16, 16]             192\n",
      "      BasicConv2d-58          [128, 96, 16, 16]               0\n",
      "           Conv2d-59         [128, 208, 16, 16]         179,712\n",
      "      BatchNorm2d-60         [128, 208, 16, 16]             416\n",
      "      BasicConv2d-61         [128, 208, 16, 16]               0\n",
      "           Conv2d-62          [128, 16, 16, 16]           7,680\n",
      "      BatchNorm2d-63          [128, 16, 16, 16]              32\n",
      "      BasicConv2d-64          [128, 16, 16, 16]               0\n",
      "           Conv2d-65          [128, 48, 16, 16]           6,912\n",
      "      BatchNorm2d-66          [128, 48, 16, 16]              96\n",
      "      BasicConv2d-67          [128, 48, 16, 16]               0\n",
      "        MaxPool2d-68         [128, 480, 16, 16]               0\n",
      "           Conv2d-69          [128, 64, 16, 16]          30,720\n",
      "      BatchNorm2d-70          [128, 64, 16, 16]             128\n",
      "      BasicConv2d-71          [128, 64, 16, 16]               0\n",
      "        Inception-72         [128, 512, 16, 16]               0\n",
      "           Conv2d-73         [128, 160, 16, 16]          81,920\n",
      "      BatchNorm2d-74         [128, 160, 16, 16]             320\n",
      "      BasicConv2d-75         [128, 160, 16, 16]               0\n",
      "           Conv2d-76         [128, 112, 16, 16]          57,344\n",
      "      BatchNorm2d-77         [128, 112, 16, 16]             224\n",
      "      BasicConv2d-78         [128, 112, 16, 16]               0\n",
      "           Conv2d-79         [128, 224, 16, 16]         225,792\n",
      "      BatchNorm2d-80         [128, 224, 16, 16]             448\n",
      "      BasicConv2d-81         [128, 224, 16, 16]               0\n",
      "           Conv2d-82          [128, 24, 16, 16]          12,288\n",
      "      BatchNorm2d-83          [128, 24, 16, 16]              48\n",
      "      BasicConv2d-84          [128, 24, 16, 16]               0\n",
      "           Conv2d-85          [128, 64, 16, 16]          13,824\n",
      "      BatchNorm2d-86          [128, 64, 16, 16]             128\n",
      "      BasicConv2d-87          [128, 64, 16, 16]               0\n",
      "        MaxPool2d-88         [128, 512, 16, 16]               0\n",
      "           Conv2d-89          [128, 64, 16, 16]          32,768\n",
      "      BatchNorm2d-90          [128, 64, 16, 16]             128\n",
      "      BasicConv2d-91          [128, 64, 16, 16]               0\n",
      "        Inception-92         [128, 512, 16, 16]               0\n",
      "           Conv2d-93         [128, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-94         [128, 128, 16, 16]             256\n",
      "      BasicConv2d-95         [128, 128, 16, 16]               0\n",
      "           Conv2d-96         [128, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-97         [128, 128, 16, 16]             256\n",
      "      BasicConv2d-98         [128, 128, 16, 16]               0\n",
      "           Conv2d-99         [128, 256, 16, 16]         294,912\n",
      "     BatchNorm2d-100         [128, 256, 16, 16]             512\n",
      "     BasicConv2d-101         [128, 256, 16, 16]               0\n",
      "          Conv2d-102          [128, 24, 16, 16]          12,288\n",
      "     BatchNorm2d-103          [128, 24, 16, 16]              48\n",
      "     BasicConv2d-104          [128, 24, 16, 16]               0\n",
      "          Conv2d-105          [128, 64, 16, 16]          13,824\n",
      "     BatchNorm2d-106          [128, 64, 16, 16]             128\n",
      "     BasicConv2d-107          [128, 64, 16, 16]               0\n",
      "       MaxPool2d-108         [128, 512, 16, 16]               0\n",
      "          Conv2d-109          [128, 64, 16, 16]          32,768\n",
      "     BatchNorm2d-110          [128, 64, 16, 16]             128\n",
      "     BasicConv2d-111          [128, 64, 16, 16]               0\n",
      "       Inception-112         [128, 512, 16, 16]               0\n",
      "          Conv2d-113         [128, 112, 16, 16]          57,344\n",
      "     BatchNorm2d-114         [128, 112, 16, 16]             224\n",
      "     BasicConv2d-115         [128, 112, 16, 16]               0\n",
      "          Conv2d-116         [128, 144, 16, 16]          73,728\n",
      "     BatchNorm2d-117         [128, 144, 16, 16]             288\n",
      "     BasicConv2d-118         [128, 144, 16, 16]               0\n",
      "          Conv2d-119         [128, 288, 16, 16]         373,248\n",
      "     BatchNorm2d-120         [128, 288, 16, 16]             576\n",
      "     BasicConv2d-121         [128, 288, 16, 16]               0\n",
      "          Conv2d-122          [128, 32, 16, 16]          16,384\n",
      "     BatchNorm2d-123          [128, 32, 16, 16]              64\n",
      "     BasicConv2d-124          [128, 32, 16, 16]               0\n",
      "          Conv2d-125          [128, 64, 16, 16]          18,432\n",
      "     BatchNorm2d-126          [128, 64, 16, 16]             128\n",
      "     BasicConv2d-127          [128, 64, 16, 16]               0\n",
      "       MaxPool2d-128         [128, 512, 16, 16]               0\n",
      "          Conv2d-129          [128, 64, 16, 16]          32,768\n",
      "     BatchNorm2d-130          [128, 64, 16, 16]             128\n",
      "     BasicConv2d-131          [128, 64, 16, 16]               0\n",
      "       Inception-132         [128, 528, 16, 16]               0\n",
      "          Conv2d-133         [128, 256, 16, 16]         135,168\n",
      "     BatchNorm2d-134         [128, 256, 16, 16]             512\n",
      "     BasicConv2d-135         [128, 256, 16, 16]               0\n",
      "          Conv2d-136         [128, 160, 16, 16]          84,480\n",
      "     BatchNorm2d-137         [128, 160, 16, 16]             320\n",
      "     BasicConv2d-138         [128, 160, 16, 16]               0\n",
      "          Conv2d-139         [128, 320, 16, 16]         460,800\n",
      "     BatchNorm2d-140         [128, 320, 16, 16]             640\n",
      "     BasicConv2d-141         [128, 320, 16, 16]               0\n",
      "          Conv2d-142          [128, 32, 16, 16]          16,896\n",
      "     BatchNorm2d-143          [128, 32, 16, 16]              64\n",
      "     BasicConv2d-144          [128, 32, 16, 16]               0\n",
      "          Conv2d-145         [128, 128, 16, 16]          36,864\n",
      "     BatchNorm2d-146         [128, 128, 16, 16]             256\n",
      "     BasicConv2d-147         [128, 128, 16, 16]               0\n",
      "       MaxPool2d-148         [128, 528, 16, 16]               0\n",
      "          Conv2d-149         [128, 128, 16, 16]          67,584\n",
      "     BatchNorm2d-150         [128, 128, 16, 16]             256\n",
      "     BasicConv2d-151         [128, 128, 16, 16]               0\n",
      "       Inception-152         [128, 832, 16, 16]               0\n",
      "       MaxPool2d-153           [128, 832, 8, 8]               0\n",
      "          Conv2d-154           [128, 256, 8, 8]         212,992\n",
      "     BatchNorm2d-155           [128, 256, 8, 8]             512\n",
      "     BasicConv2d-156           [128, 256, 8, 8]               0\n",
      "          Conv2d-157           [128, 160, 8, 8]         133,120\n",
      "     BatchNorm2d-158           [128, 160, 8, 8]             320\n",
      "     BasicConv2d-159           [128, 160, 8, 8]               0\n",
      "          Conv2d-160           [128, 320, 8, 8]         460,800\n",
      "     BatchNorm2d-161           [128, 320, 8, 8]             640\n",
      "     BasicConv2d-162           [128, 320, 8, 8]               0\n",
      "          Conv2d-163            [128, 32, 8, 8]          26,624\n",
      "     BatchNorm2d-164            [128, 32, 8, 8]              64\n",
      "     BasicConv2d-165            [128, 32, 8, 8]               0\n",
      "          Conv2d-166           [128, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-167           [128, 128, 8, 8]             256\n",
      "     BasicConv2d-168           [128, 128, 8, 8]               0\n",
      "       MaxPool2d-169           [128, 832, 8, 8]               0\n",
      "          Conv2d-170           [128, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-171           [128, 128, 8, 8]             256\n",
      "     BasicConv2d-172           [128, 128, 8, 8]               0\n",
      "       Inception-173           [128, 832, 8, 8]               0\n",
      "          Conv2d-174           [128, 384, 8, 8]         319,488\n",
      "     BatchNorm2d-175           [128, 384, 8, 8]             768\n",
      "     BasicConv2d-176           [128, 384, 8, 8]               0\n",
      "          Conv2d-177           [128, 192, 8, 8]         159,744\n",
      "     BatchNorm2d-178           [128, 192, 8, 8]             384\n",
      "     BasicConv2d-179           [128, 192, 8, 8]               0\n",
      "          Conv2d-180           [128, 384, 8, 8]         663,552\n",
      "     BatchNorm2d-181           [128, 384, 8, 8]             768\n",
      "     BasicConv2d-182           [128, 384, 8, 8]               0\n",
      "          Conv2d-183            [128, 48, 8, 8]          39,936\n",
      "     BatchNorm2d-184            [128, 48, 8, 8]              96\n",
      "     BasicConv2d-185            [128, 48, 8, 8]               0\n",
      "          Conv2d-186           [128, 128, 8, 8]          55,296\n",
      "     BatchNorm2d-187           [128, 128, 8, 8]             256\n",
      "     BasicConv2d-188           [128, 128, 8, 8]               0\n",
      "       MaxPool2d-189           [128, 832, 8, 8]               0\n",
      "          Conv2d-190           [128, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-191           [128, 128, 8, 8]             256\n",
      "     BasicConv2d-192           [128, 128, 8, 8]               0\n",
      "       Inception-193          [128, 1024, 8, 8]               0\n",
      "AdaptiveAvgPool2d-194          [128, 1024, 1, 1]               0\n",
      "         Dropout-195                [128, 1024]               0\n",
      "          Linear-196                   [128, 1]           1,025\n",
      "================================================================\n",
      "Total params: 5,600,929\n",
      "Trainable params: 5,600,929\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 96.00\n",
      "Forward/backward pass size (MB): 15731.00\n",
      "Params size (MB): 21.37\n",
      "Estimated Total Size (MB): 15848.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load in the pretrained densenet model and change the number of outputs on the final fc layer\n",
    "model_googlenet = torchvision.models.googlenet(pretrained=True)\n",
    "model_googlenet.fc = nn.Linear(model_googlenet.fc.in_features, 1)\n",
    "\n",
    "# get a summary of the model based on our input shape\n",
    "model_googlenet.to(device)\n",
    "summary(model_googlenet, img.shape, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-catalyst",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we have the data loader created and the model defined, let's define our loss and optimiser and run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prepared-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "class LoggedVariable:\n",
    "    def __init__(self):\n",
    "        self.batch_values = []\n",
    "        self.epoch_values = []\n",
    "        \n",
    "    def append(self, value):\n",
    "        self.batch_values.append(value)\n",
    "        \n",
    "    def end_epoch(self):\n",
    "        mean_batch_values = mean(self.batch_values)\n",
    "        self.epoch_values.append(mean_batch_values)\n",
    "        self.batch_values = []\n",
    "        \n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.variables = {}\n",
    "    \n",
    "    def __call__(self, key, value):\n",
    "        if key not in self.variables:\n",
    "            self.variables[key] = LoggedVariable()\n",
    "        self.variables[key].append(value)\n",
    "    \n",
    "    def end_epoch(self, epoch):\n",
    "        print(f\"end epoch {epoch}\", end = '')\n",
    "        for key, val in self.variables.items():\n",
    "            val.end_epoch()\n",
    "            print(f\" {key}: {val.epoch_values[epoch]:.2f}\", end = '')\n",
    "        print()\n",
    "    \n",
    "    def history(self):\n",
    "        return { k:v.epoch_values for k, v in self.variables.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "descending-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores, targets):\n",
    "    _, predictions = scores.max(1)\n",
    "    num_correct = (predictions == targets).sum()\n",
    "    num_samples = predictions.size(0)\n",
    "    return num_correct / num_samples\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "elect-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def fit(model, optimizer_class, criterion, train_loader, valid_loader, device, epochs=20, learning_rate=0.001):\n",
    "    print(f'fitting model: {type(model).__name__} for {epochs} epochs.')\n",
    "    \n",
    "    # set up the optimizer\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # initalise stats\n",
    "    log = Logger()\n",
    "    start_time_sec = time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train and evaluate on the training set\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # put X and y for the batch on the GPU is possible\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            \n",
    "            # forward pass\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets.unsqueeze(1).float())\n",
    "            \n",
    "            # backwards pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # gradient descent\n",
    "            optimizer.step()\n",
    "            \n",
    "            # log the metrics\n",
    "            acc = binary_acc(scores, targets.unsqueeze(1))\n",
    "            log('train_acc', acc.item())\n",
    "            log('train_loss', loss.item())\n",
    "            \n",
    "            print('\\r', f'train.\\t\\tepoch: {epoch}\\tbatch: {batch_idx + 1}/{len(train_loader)}\\tloss: {loss:.3f}\\taccuracy: {acc} ', sep='', end='', flush=True)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # evaluate on the validation set\n",
    "        model.eval()\n",
    "        for batch_idx, (data, targets) in enumerate(valid_loader):\n",
    "            # put X and y for the batch on the device\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            \n",
    "            # compute the predictions\n",
    "            scores = model(data)\n",
    "            \n",
    "            # computer the metric and log them\n",
    "            loss = criterion(scores, targets.unsqueeze(1).float())\n",
    "            acc = binary_acc(scores, targets.unsqueeze(1))\n",
    "            log('valid_acc', acc.item())\n",
    "            log('valid_loss', loss.item())\n",
    "        \n",
    "            print('\\r', f'validate.\\tepoch: {epoch}\\tbatch: {batch_idx + 1}/{len(valid_loader)}\\tloss: {loss:.3f}\\taccuracy: {acc} ', sep='', end='', flush=True)        \n",
    "        \n",
    "        print()\n",
    "        \n",
    "        save_checkpoint(epoch, model, optimizer, experiment_root / f\"{type(model).__name__}_checkpoint_{epoch}.ckpt\") \n",
    "        log.end_epoch(epoch)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end_time_sec       = time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print(\"training complete.\")\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "    print()\n",
    "    \n",
    "    return log.history()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "operating-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()  # combines a Sigmoid layer and the BCELoss in one single class\n",
    "# history = fit(model_vgg16, optim.Adam, criterion, train_loader, valid_loader, device, epochs=num_epochs, learning_rate=learning_rate)\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "therapeutic-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model: ResNet for 30 epochs.\n",
      "train.\t\tepoch: 0\tbatch: 547/547\tloss: 0.408\taccuracy: 82.0 \n",
      "validate.\tepoch: 0\tbatch: 235/235\tloss: 0.344\taccuracy: 85.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_0.ckpt\n",
      "end epoch 0 train_acc: 83.09 train_loss: 0.39 valid_acc: 75.20 valid_loss: 0.55\n",
      "\n",
      "train.\t\tepoch: 1\tbatch: 547/547\tloss: 0.221\taccuracy: 91.0 \n",
      "validate.\tepoch: 1\tbatch: 235/235\tloss: 0.269\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_1.ckpt\n",
      "end epoch 1 train_acc: 86.31 train_loss: 0.33 valid_acc: 86.26 valid_loss: 0.34\n",
      "\n",
      "train.\t\tepoch: 2\tbatch: 547/547\tloss: 0.293\taccuracy: 89.0 \n",
      "validate.\tepoch: 2\tbatch: 235/235\tloss: 0.468\taccuracy: 85.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_2.ckpt\n",
      "end epoch 2 train_acc: 87.86 train_loss: 0.29 valid_acc: 87.66 valid_loss: 0.31\n",
      "\n",
      "train.\t\tepoch: 3\tbatch: 547/547\tloss: 0.298\taccuracy: 87.0 \n",
      "validate.\tepoch: 3\tbatch: 235/235\tloss: 0.633\taccuracy: 77.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_3.ckpt\n",
      "end epoch 3 train_acc: 88.80 train_loss: 0.28 valid_acc: 78.17 valid_loss: 0.57\n",
      "\n",
      "train.\t\tepoch: 4\tbatch: 547/547\tloss: 0.210\taccuracy: 92.0 \n",
      "validate.\tepoch: 4\tbatch: 235/235\tloss: 0.342\taccuracy: 81.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_4.ckpt\n",
      "end epoch 4 train_acc: 89.44 train_loss: 0.26 valid_acc: 87.55 valid_loss: 0.33\n",
      "\n",
      "train.\t\tepoch: 5\tbatch: 547/547\tloss: 0.337\taccuracy: 86.0 \n",
      "validate.\tepoch: 5\tbatch: 235/235\tloss: 0.583\taccuracy: 81.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_5.ckpt\n",
      "end epoch 5 train_acc: 90.14 train_loss: 0.25 valid_acc: 86.19 valid_loss: 0.37\n",
      "\n",
      "train.\t\tepoch: 6\tbatch: 547/547\tloss: 0.201\taccuracy: 93.0 \n",
      "validate.\tepoch: 6\tbatch: 235/235\tloss: 0.297\taccuracy: 90.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_6.ckpt\n",
      "end epoch 6 train_acc: 90.61 train_loss: 0.23 valid_acc: 88.74 valid_loss: 0.36\n",
      "\n",
      "train.\t\tepoch: 7\tbatch: 547/547\tloss: 0.255\taccuracy: 94.0 \n",
      "validate.\tepoch: 7\tbatch: 235/235\tloss: 0.155\taccuracy: 94.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_7.ckpt\n",
      "end epoch 7 train_acc: 91.09 train_loss: 0.23 valid_acc: 90.55 valid_loss: 0.27\n",
      "\n",
      "train.\t\tepoch: 8\tbatch: 547/547\tloss: 0.215\taccuracy: 87.0 \n",
      "validate.\tepoch: 8\tbatch: 235/235\tloss: 0.254\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_8.ckpt\n",
      "end epoch 8 train_acc: 91.52 train_loss: 0.22 valid_acc: 89.40 valid_loss: 0.33\n",
      "\n",
      "train.\t\tepoch: 9\tbatch: 547/547\tloss: 0.182\taccuracy: 90.0 \n",
      "validate.\tepoch: 9\tbatch: 235/235\tloss: 0.265\taccuracy: 90.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_9.ckpt\n",
      "end epoch 9 train_acc: 91.92 train_loss: 0.21 valid_acc: 88.93 valid_loss: 0.31\n",
      "\n",
      "train.\t\tepoch: 10\tbatch: 547/547\tloss: 0.152\taccuracy: 96.0 \n",
      "validate.\tepoch: 10\tbatch: 235/235\tloss: 0.216\taccuracy: 94.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_10.ckpt\n",
      "end epoch 10 train_acc: 92.34 train_loss: 0.20 valid_acc: 90.63 valid_loss: 0.26\n",
      "\n",
      "train.\t\tepoch: 11\tbatch: 547/547\tloss: 0.181\taccuracy: 94.0 \n",
      "validate.\tepoch: 11\tbatch: 235/235\tloss: 0.170\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_11.ckpt\n",
      "end epoch 11 train_acc: 92.53 train_loss: 0.19 valid_acc: 90.26 valid_loss: 0.35\n",
      "\n",
      "train.\t\tepoch: 12\tbatch: 547/547\tloss: 0.209\taccuracy: 93.0 \n",
      "validate.\tepoch: 12\tbatch: 235/235\tloss: 0.096\taccuracy: 98.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_12.ckpt\n",
      "end epoch 12 train_acc: 93.07 train_loss: 0.18 valid_acc: 90.60 valid_loss: 0.29\n",
      "\n",
      "train.\t\tepoch: 13\tbatch: 547/547\tloss: 0.213\taccuracy: 93.0 \n",
      "validate.\tepoch: 13\tbatch: 235/235\tloss: 0.725\taccuracy: 83.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_13.ckpt\n",
      "end epoch 13 train_acc: 93.69 train_loss: 0.17 valid_acc: 86.98 valid_loss: 0.44\n",
      "\n",
      "train.\t\tepoch: 14\tbatch: 547/547\tloss: 0.114\taccuracy: 95.0 \n",
      "validate.\tepoch: 14\tbatch: 235/235\tloss: 0.426\taccuracy: 88.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_14.ckpt\n",
      "end epoch 14 train_acc: 94.33 train_loss: 0.15 valid_acc: 88.91 valid_loss: 0.36\n",
      "\n",
      "train.\t\tepoch: 15\tbatch: 547/547\tloss: 0.172\taccuracy: 96.0 \n",
      "validate.\tepoch: 15\tbatch: 235/235\tloss: 0.485\taccuracy: 90.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_15.ckpt\n",
      "end epoch 15 train_acc: 95.07 train_loss: 0.13 valid_acc: 88.40 valid_loss: 0.38\n",
      "\n",
      "train.\t\tepoch: 16\tbatch: 547/547\tloss: 0.046\taccuracy: 98.0 \n",
      "validate.\tepoch: 16\tbatch: 235/235\tloss: 0.523\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_16.ckpt\n",
      "end epoch 16 train_acc: 95.87 train_loss: 0.11 valid_acc: 88.86 valid_loss: 0.45\n",
      "\n",
      "train.\t\tepoch: 17\tbatch: 547/547\tloss: 0.069\taccuracy: 97.0  \n",
      "validate.\tepoch: 17\tbatch: 235/235\tloss: 0.237\taccuracy: 90.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_17.ckpt\n",
      "end epoch 17 train_acc: 96.80 train_loss: 0.09 valid_acc: 89.87 valid_loss: 0.36\n",
      "\n",
      "train.\t\tepoch: 18\tbatch: 547/547\tloss: 0.085\taccuracy: 96.0  \n",
      "validate.\tepoch: 18\tbatch: 235/235\tloss: 0.139\taccuracy: 94.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_18.ckpt\n",
      "end epoch 18 train_acc: 97.41 train_loss: 0.07 valid_acc: 89.18 valid_loss: 0.42\n",
      "\n",
      "train.\t\tepoch: 19\tbatch: 547/547\tloss: 0.035\taccuracy: 99.0  \n",
      "validate.\tepoch: 19\tbatch: 235/235\tloss: 0.371\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_19.ckpt\n",
      "end epoch 19 train_acc: 97.97 train_loss: 0.05 valid_acc: 89.35 valid_loss: 0.48\n",
      "\n",
      "train.\t\tepoch: 20\tbatch: 547/547\tloss: 0.015\taccuracy: 100.0 \n",
      "validate.\tepoch: 20\tbatch: 235/235\tloss: 0.487\taccuracy: 90.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_20.ckpt\n",
      "end epoch 20 train_acc: 98.16 train_loss: 0.05 valid_acc: 86.35 valid_loss: 0.74\n",
      "\n",
      "train.\t\tepoch: 21\tbatch: 547/547\tloss: 0.026\taccuracy: 99.0  \n",
      "validate.\tepoch: 21\tbatch: 235/235\tloss: 0.589\taccuracy: 94.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_21.ckpt\n",
      "end epoch 21 train_acc: 98.49 train_loss: 0.04 valid_acc: 89.70 valid_loss: 0.58\n",
      "\n",
      "train.\t\tepoch: 22\tbatch: 547/547\tloss: 0.082\taccuracy: 96.0  \n",
      "validate.\tepoch: 22\tbatch: 235/235\tloss: 0.360\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_22.ckpt\n",
      "end epoch 22 train_acc: 98.67 train_loss: 0.03 valid_acc: 85.78 valid_loss: 0.84\n",
      "\n",
      "train.\t\tepoch: 23\tbatch: 547/547\tloss: 0.009\taccuracy: 100.0 \n",
      "validate.\tepoch: 23\tbatch: 235/235\tloss: 1.002\taccuracy: 85.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_23.ckpt\n",
      "end epoch 23 train_acc: 98.85 train_loss: 0.03 valid_acc: 88.89 valid_loss: 0.72\n",
      "\n",
      "train.\t\tepoch: 24\tbatch: 547/547\tloss: 0.005\taccuracy: 100.0 \n",
      "validate.\tepoch: 24\tbatch: 235/235\tloss: 0.357\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_24.ckpt\n",
      "end epoch 24 train_acc: 98.91 train_loss: 0.03 valid_acc: 89.14 valid_loss: 0.66\n",
      "\n",
      "train.\t\tepoch: 25\tbatch: 547/547\tloss: 0.077\taccuracy: 97.0  \n",
      "validate.\tepoch: 25\tbatch: 235/235\tloss: 0.714\taccuracy: 85.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_25.ckpt\n",
      "end epoch 25 train_acc: 98.87 train_loss: 0.03 valid_acc: 87.19 valid_loss: 0.74\n",
      "\n",
      "train.\t\tepoch: 26\tbatch: 547/547\tloss: 0.005\taccuracy: 100.0 \n",
      "validate.\tepoch: 26\tbatch: 235/235\tloss: 1.527\taccuracy: 85.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_26.ckpt\n",
      "end epoch 26 train_acc: 99.09 train_loss: 0.02 valid_acc: 87.76 valid_loss: 0.75\n",
      "\n",
      "train.\t\tepoch: 27\tbatch: 547/547\tloss: 0.025\taccuracy: 99.0  \n",
      "validate.\tepoch: 27\tbatch: 235/235\tloss: 1.857\taccuracy: 83.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_27.ckpt\n",
      "end epoch 27 train_acc: 99.22 train_loss: 0.02 valid_acc: 88.75 valid_loss: 0.72\n",
      "\n",
      "train.\t\tepoch: 28\tbatch: 547/547\tloss: 0.020\taccuracy: 98.0  \n",
      "validate.\tepoch: 28\tbatch: 235/235\tloss: 0.641\taccuracy: 92.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_28.ckpt\n",
      "end epoch 28 train_acc: 99.02 train_loss: 0.02 valid_acc: 90.33 valid_loss: 0.66\n",
      "\n",
      "train.\t\tepoch: 29\tbatch: 547/547\tloss: 0.047\taccuracy: 99.0  \n",
      "validate.\tepoch: 29\tbatch: 235/235\tloss: 1.269\taccuracy: 83.0 \n",
      "saving checkpoint to /home/ubuntu/pathgen/experiments/all/ResNet_checkpoint_29.ckpt\n",
      "end epoch 29 train_acc: 99.28 train_loss: 0.02 valid_acc: 89.43 valid_loss: 0.67\n",
      "\n",
      "training complete.\n",
      "Time total:     4675.00 sec\n",
      "Time per epoch: 155.83 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# models = [model_vgg16, model_resnet18, model_googlenet]\n",
    "models = [model_resnet18]\n",
    "for model in models:\n",
    "    # fit the model\n",
    "    history = fit(model, optim.Adam, criterion, train_loader, valid_loader, device, epochs=num_epochs)\n",
    "    \n",
    "    # save the results\n",
    "    json.dump(history, open(experiment_root / f\"results_{type(model).__name__}.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-rating",
   "metadata": {},
   "source": [
    "Now let's visualise the training process and see how things went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-excellence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "actual-camcorder",
   "metadata": {},
   "source": [
    "Let's test the model on the test set to see how it performs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathgen",
   "language": "python",
   "name": "pathgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
